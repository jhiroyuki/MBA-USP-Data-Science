# contando a quantidade de linhas
sdf_nrow(dados_voos_completo_parquet)
# verificar o tempo de processamento para coletar os dados armazenados em parquet
system.time(spark_read_parquet(sc,
name = "dados_voos_completo_parquet",
path = "./datasets/arquivos-comprimidos/voos-avioes.parquet"))
dados_voos_completo_parquet <- dados_voos_completo_parquet %>%
mutate(ANO = as.character(year(FL_DATE)))
# verificando a variavel adicionada
glimpse(dados_voos_completo_parquet)
# verificando a variável adicionada
glimpse(dados_voos_completo_parquet)
# carregando o dataset em um diretório local e particionando os dados por ano
spark_write_parquet(dados_voos_completo_parquet,
path = "./datasets/arquivos-comprimidos/voos-avioes-anos.parquet",
partition_by = c("ANO"),
mode = "overwrite")
# verificar o tempo de processamento para coletar os dados armazenados em parquet do ano de 2017
system.time(dados_voos_ano_2017_parquet <- spark_read_parquet(sc,
name = "dados_voos_ano_2017_parquet",
path = "./datasets/arquivos-comprimidos/voos-avioes-anos.parquet/ANO=2017"))
# mesmo procedimento para acessar os dados do ano de 2018
system.time(dados_voos_ano_2018_parquet <- spark_read_parquet(sc,
name = "dados_voos_ano_2018_parquet",
path = "./datasets/arquivos-comprimidos/voos-avioes-anos.parquet/ANO=2018"))
dados_voos_completo_parquet <- dados_voos_completo_parquet %>%
mutate(MES = as.character(month(FL_DATE)))
# e possivel adicionar mais particoes quando o arquivo parquet e salvo em algum diretorio
spark_write_parquet(dados_voos_completo_parquet,
path = "./datasets/arquivos-comprimidos/voos-avioes-anos-mes.parquet",
partition_by = c("ANO","MES"),
mode = "overwrite")
# desconectar alguma conexão ativa com o spark
spark_disconnect_all()
# fun??o para instalar e carregar os pacotes necess?rios
instalar_carregar_pacotes <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
# lista de pacotes necess?rios
pacotes <- c("sparklyr",
"dplyr",
"arrow",
"nlme",
"ggplot2",
"carrier",
"mlflow",
"reticulate",
"stats",
"glue")
# instalar e carregar os pacotes
instalar_carregar_pacotes(pacotes)
# fun??o para instalar e carregar os pacotes necess?rios
instalar_carregar_pacotes <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
# lista de pacotes necess?rios
pacotes <- c("sparklyr",
"dplyr",
"arrow",
"nlme",
"ggplot2",
"carrier",
"mlflow",
"reticulate",
"stats",
"glue")
# instalar e carregar os pacotes
instalar_carregar_pacotes(pacotes)
# desconectar alguma conexão ativa com o spark
spark_disconnect_all()
# conectar ao cluster spark local
sc <- spark_connect(master = "local",
spark_home="./spark/build/spark-3.4.0-bin-hadoop3")
# carregando o primeiro dataset da base de dados de voos de avioes
dados_voos_2017 <- spark_read_csv(sc,
name="dados_voos_2017",
path="./datasets/voos-avioes/2017.csv",
header=TRUE,
infer_schema=TRUE)
# verificando nossos dados
glimpse(dados_voos_2017)
# carregando o primeiro dataset da base de dados de voos de avioes
dados_voos_2017 <- spark_read_csv(sc,
name="dados_voos_2017",
path="./datasets/voos-avioes/2017.csv",
header=TRUE,
infer_schema=TRUE)
# conectar ao cluster spark local
sc <- spark_connect(master = "local",
spark_home="./spark/build/spark-3.4.0-bin-hadoop3")
# carregando o primeiro dataset da base de dados de voos de avioes
dados_voos_2017 <- spark_read_csv(sc,
name="dados_voos_2017",
path="./datasets/voos-avioes/2017.csv",
header=TRUE,
infer_schema=TRUE)
# verificando nossos dados
glimpse(dados_voos_2017)
# contagem de linhas do dataset
sdf_nrow(dados_voos_2017)
# verificar o tempo de processamento para coletar os dados do dataset dos dados de 2017
system.time(dados_voos_2017 <- spark_read_csv(sc,
name="dados_voos_2017",
path="./datasets/voos-avioes/2017.csv",
header=TRUE,
infer_schema=TRUE))
# quando um dataset é lido pelo spark, ele é carregado de forma distribuída na memória
# elas ainda não foram entregues para o driver (R Studio), essas são as operações de
# de transformações de dados
apenas_voos_cancelados <- dados_voos_2017 %>% filter(CANCELLED == 1)
# as operaçoes de manipulações são feitas distruídas e com execução realizada quando uma ação é chamada
# ação: mostrar os dados
glimpse(apenas_voos_cancelados)
# criando uma variavel que sinaliza que o voo esta atrasado quando
# o tempo de atraso na chegada for maior que 15 minutos
dados_voos_2017 <- dados_voos_2017 %>%
mutate(ATRASADO = ifelse(ARR_DELAY >= 15, 1, 0))
# verificando a variavel criada
glimpse(dados_voos_2017)
# exemplo de uma operação de transformação e guardar os dados em uma variável
# contagem da quantidade de voos atrasados e não atrasados
dados_agrupados_voos_atrasados <- dados_voos_2017 %>%
group_by(ATRASADO) %>%
summarize(QUANTIDADE = n()) %>%
mutate(PORCENTAGEM = QUANTIDADE/sum(QUANTIDADE)*100)
# outra ação de visualização
glimpse(dados_agrupados_voos_atrasados)
# o dataframe spark pode ser transferido da memória do R e transformado em um R dataframe com
# a função collect nessa operação, os dados distribuídos vão todos para o driver na aplicação
# e faz a transformação
dados_agrupados_voos_atrasados_R <- collect(dados_agrupados_voos_atrasados)
# R dataframe, fora do cluster spark
dados_agrupados_voos_atrasados_R
# verificando diferença entre quantidade de voos atrasados e voos não atrasados de 2017
ggplot(data=dados_agrupados_voos_atrasados_R, aes(x=as.character(ATRASADO), y=QUANTIDADE)) +
geom_bar(stat="identity", fill="steelblue")+
geom_text(aes(label=QUANTIDADE), vjust=1.6, color="white", size=3.5)+
xlab("Voos atrasados") +
ylab("Quantidade") +
ggtitle("Quantidade de voos atrasados em 2017") +
theme_minimal()
# verificando os missing values
dados_voos_2017 %>% filter(is.na(ATRASADO))
# levando o dataframe R para o cluster spark
dados_agrupados_voos_atrasados_spark <- copy_to(sc,
dados_agrupados_voos_atrasados_R,
"dados_agrupados_voos_atrasados_spark")
# verificando o dataframe spark
glimpse(dados_agrupados_voos_atrasados_spark)
# mostrando a lista de tabelas do cluster spark
src_tbls(sc)
# lendo os dados de 2018 do mesmo esquema de dataset como um dataframe Spark
dados_voos_2018 <- spark_read_csv(sc,
name="dados_voos_2018",
path="./datasets/voos-avioes/2018.csv",
header=TRUE,
infer_schema=TRUE)
# visualizando os dados de 2018
glimpse(dados_voos_2018)
# criando uma variável que sinaliza que o voo está atrasado quando o tempo de atraso
# na partida for maior que 15 minutos também para os dados de de 2018
dados_voos_2018 <- dados_voos_2018 %>%
mutate(ATRASADO = ifelse(ARR_DELAY >= 15, 1, 0))
# exemplo de uma operação de transformação e guardar os dados em uma variável
# contagem da quantidade de voos atrasados e não atrasados
dados_agrupados_voos_atrasados_218 <- dados_voos_2018 %>%
group_by(ATRASADO) %>%
summarize(QUANTIDADE = n()) %>%
mutate(PORCENTAGEM = QUANTIDADE/sum(QUANTIDADE)*100)
# exemplo de uma operação de transformação e guardar os dados em uma variável
# contagem da quantidade de voos atrasados e não atrasados
dados_agrupados_voos_atrasados_2018 <- dados_voos_2018 %>%
group_by(ATRASADO) %>%
summarize(QUANTIDADE = n()) %>%
mutate(PORCENTAGEM = QUANTIDADE/sum(QUANTIDADE)*100)
# verificando as porcentagens de voos atrasados em 2017
dados_agrupados_voos_atrasados
# verificando as porcentagens de voos atrasados em 2018
dados_agrupados_voos_atrasados_2018
# verificando diferença entre quantidade de voos cancelados e voos não cancelados de 2018
ggplot(data=collect(dados_agrupados_voos_atrasados_218), aes(x=as.character(ATRASADO), y=QUANTIDADE)) +
geom_bar(stat="identity", fill="steelblue")+
geom_text(aes(label=QUANTIDADE), vjust=1.6, color="white", size=3.5)+
xlab("Voos atrasados") +
ylab("Quantidade") +
ggtitle("Quantidade de voos atrasados em 2018") +
theme_minimal()
# unindo os dois datasets: dados de 2017 e 2018
dados_voos_completos <- union_all(dados_voos_2017, dados_voos_2018)
# continua as mesmas informacões
glimpse(dados_voos_completos)
# verificando a quantidade de linhas do dataset
sdf_nrow(dados_voos_completos)
# criar um objeto de fração com valor bem pequeno (Ex. 1% dos dados)
fracao_contagem <- 0.01
# coleta uma pequena amostra do dataset completo
pequena_amostra <- sdf_sample(dados_voos_completos,
fraction = fracao_contagem,
replacement = TRUE,
seed = 42)
# contagem de linhas do dataset
numero_de_linhas <- sdf_nrow(pequena_amostra)
# aproximacao contagem
aproximacao_contagem_linhas <- numero_de_linhas/fracao_contagem
# valores aproximado da quantidade de linhas do dataset
print(aproximacao_contagem_linhas)
# contagem de linhas do dataset
diferenca <- sdf_nrow(dados_voos_completos) - aproximacao_contagem_linhas
# diferenca computada
print(diferenca)
# selecionando apenas as colunas que sao importantes para o modelo
dados_selecionados <- select(dados_voos_completos, ACTUAL_ELAPSED_TIME, DISTANCE)
View(dados_selecionados)
# retirar missing values do dataset caso existam
dados_selecionados <- dados_selecionados %>%
na.omit
# verificando a correlacao
ml_corr(dados_selecionados,
columns = c("ACTUAL_ELAPSED_TIME","DISTANCE"),
method = "pearson")
# coletando uma pequena amostra
pequena_amostra <- sdf_sample(dados_selecionados, fraction = 0.01, seed = 42)
# contando a quantidade de linhas da amostra (1%)
sdf_nrow(pequena_amostra)
# verificando a correlacao das duas variaveis no grafico de dispersao
ggplot(pequena_amostra, aes(x=DISTANCE, y=ACTUAL_ELAPSED_TIME)) +
geom_point(alpha = 0.1) +
geom_smooth(method=lm, color="red", se=FALSE) +
ylab("Distancia percorrida") +
xlab("Tempo total decorrido") +
theme_minimal()
# realizando o fit do modelo
modelo_linear <- dados_selecionados %>%
ml_generalized_linear_regression(ACTUAL_ELAPSED_TIME ~ DISTANCE)
# realizando o fit do modelo
modelo_linear <- dados_selecionados %>%
ml_generalized_linear_regression(ACTUAL_ELAPSED_TIME ~ DISTANCE)
# relatorio do modelo
summary(modelo_linear)
# realizando o fit do modelo utilizando a função ml_linear_regression
dados_selecionados %>%
ml_linear_regression(ACTUAL_ELAPSED_TIME ~ DISTANCE) %>%
summary()
# coletando os fitted values
dados_selecionados <- ml_predict(modelo_linear, dados_selecionados)
# verificando os fitted values no dataset
head(dados_selecionados)
# verificando a correlação ao quadrado entre valores observados e fitted values
ml_corr(dados_selecionados,
columns = c("ACTUAL_ELAPSED_TIME","prediction"),
method = "pearson")^2
# coletando os resíduos
residuos <- sdf_residuals(modelo_linear)
# verificando os resíduos coletados
head(residuos)
# coletando amostra para verificar a distribuição dos resíduos
amostra_residuos <- sdf_sample(residuos, fraction = 0.01) %>%
collect
# visualizando a distribuição dos resíduos
ggplot(amostra_residuos, aes(x = residuals)) +
geom_histogram(aes(y = ..density..), binwidth = 5) +
ylab("Frequencia") +
xlab("Residuos") +
theme_minimal() +
geom_density()
# criando um R dataframe
dados_novos <- data.frame(DISTANCE=c(200))
# criando um R dataframe
dados_novos <- data.frame(DISTANCE=c(2000))
# transformando o datafram R em um dataframe Spark
dados_novos_spark <- copy_to(sc, dados_novos, "dados_novos_spark")
# qual o tempo total de viagem quando a distância é igual a 2000 km?
resultado <- ml_predict(modelo_linear, dados_novos_spark)
# previsão do modelo
print(resultado)
# lembrando que o resultado e um dataframe spark, para coletá-lo precisa usar a função collect
resultado_R <- resultado %>%
collect
# previsão como um dataframe R
print(resultado_R$prediction)
# revisitando nosso dataset completo
glimpse(dados_voos_completos)
# adicionando variáveis de tempo para verificar as interferências dessas variáveis nos atrasos dos voos
dados_voos_completos_tratado <- dados_voos_completos %>%
mutate(ATRASO_PARTIDA = as.numeric(DEP_DELAY),
ATRASO_CHEGADA = as.numeric(ARR_DELAY),
MES = as.character(month(FL_DATE)),
DIA_DA_SEMANA = as.character(weekday(FL_DATE)))
# coletando uma pequena amostra para verificar essas variáveis graficamente
amostra_voos_completos_tratado_R <- dados_voos_completos_tratado %>%
group_by(DIA_DA_SEMANA) %>%
summarise(media_atraso_chegada = mean(ATRASO_CHEGADA)) %>%
collect
# verificando a variacao da media de atraso na chegada dos voos em relacao ao dia da semana
ggplot(data=amostra_voos_completos_tratado_R, aes(x=as.character(DIA_DA_SEMANA), y=sprintf("%0.2f", round(media_atraso_chegada, digits = 2)))) +
geom_bar(stat="identity", fill="steelblue") +
xlab("Dia da semana") +
ylab("Media de atraso dos voos") +
ggtitle("Media de atraso dos voos por dia da semana") +
theme_minimal()
# coletando uma pequena amostra para verificar essas variacoes graficamente agora nos meses
amostra_voos_completos_tratado_R <- dados_voos_completos_tratado %>%
group_by(MES) %>%
summarise(media_atraso_chegada = mean(ATRASO_CHEGADA)) %>%
collect
# verificando a variação da média de atraso na chegada dos voos em relaçã ao mês do ano
ggplot(data=amostra_voos_completos_tratado_R, aes(x=fct_inorder(as.factor(MES)), y=sprintf("%0.2f", round(media_atraso_chegada, digits = 2)))) +
geom_bar(stat="identity", fill="steelblue") +
xlab("Mes") +
ylab("Media de atraso dos voos") +
ggtitle("Media de atraso dos voos por mes") +
theme_minimal()
# coletando uma pequena amostra para verificar essas variacoes graficamente agora nos meses
amostra_voos_completos_tratado_R <- dados_voos_completos_tratado %>%
group_by(MES) %>%
summarise(media_atraso_chegada = mean(ATRASO_CHEGADA)) %>%
collect
# verificando a variação da média de atraso na chegada dos voos em relaçã ao mês do ano
ggplot(data=amostra_voos_completos_tratado_R, aes(x=fct_inorder(as.factor(MES)), y=sprintf("%0.2f", round(media_atraso_chegada, digits = 2)))) +
geom_bar(stat="identity", fill="steelblue") +
xlab("Mes") +
ylab("Media de atraso dos voos") +
ggtitle("Media de atraso dos voos por mes") +
theme_minimal()
# coletando uma pequena amostra para verificar essas variacoes graficamente agora nos meses
amostra_voos_completos_tratado_R <- dados_voos_completos_tratado %>%
group_by(MES) %>%
summarise(media_atraso_chegada = mean(ATRASO_CHEGADA)) %>%
collect
# verificando a variação da média de atraso na chegada dos voos em relaçã ao mês do ano
ggplot(data=amostra_voos_completos_tratado_R, aes(x=as.factor(MES), y=sprintf("%0.2f", round(media_atraso_chegada, digits = 2)))) +
geom_bar(stat="identity", fill="steelblue") +
xlab("Mes") +
ylab("Media de atraso dos voos") +
ggtitle("Media de atraso dos voos por mes") +
theme_minimal()
# selecionando as variáveis da modelagem
dados_modelo_atraso <- select(dados_voos_completos_tratado, ATRASADO, CRS_DEP_TIME, MES, DIA_DA_SEMANA, DISTANCE)
# verificando minhas variáveis de entrada do modelo
glimpse(dados_modelo_atraso)
# adicionando a variável de horas nos dados de entrada do modelo baseado na coluna CRS_DEP_TIME
dados_modelo_atraso <- dados_modelo_atraso %>%
ft_bucketizer(
input_col = "CRS_DEP_TIME",
output_col = "HORAS",
splits = seq(0, 2400, 400)
)
# visualizando como ficaram as nossas variáveis de entrada
glimpse(dados_modelo_atraso)
# retirando os missing values do modelo
dados_modelo_atraso_tratados <- dados_modelo_atraso %>%
na.omit
# realizando o fit do modelo logística para a variável y ATRASADO
modelo_atrasos_voos <- ml_generalized_linear_regression(dados_modelo_atraso_tratados,
ATRASADO ~ DIA_DA_SEMANA + HORAS + MES + DISTANCE,
famlily="binomial"
)
# resumo dos modelos e seus coeficientes
summary(modelo_atrasos_voos)
# coletando os fitted values
valores_predicts <- ml_predict(modelo_atrasos_voos, dados_modelo_atraso_tratados)
# visualizando as primeiras linhas do dataset
head(valores_predicts)
teste <- valores_predicts %>% filter(prediction >= 0.5)
head(teste)
# visualizando as primeiras linhas do dataset
head(valores_predicts)
# adotando o threshold de 0.5 para coletar os labels dos eventos e não eventos
valores_predicts <- valores_predicts %>%
mutate(y_hat = ifelse(prediction >= 0.5, 1, 0))
# verificando os labels
head(valores_predicts)
# coleta da performance do modelo via AUC (Area abaixo da curva ROC)
ml_binary_classification_evaluator(valores_predicts,
label_col = "ATRASADO",
metric_name = "areaUnderROC",
raw_prediction_col ="prediction")
# verificando a matriz de confusao em grandes volumes de dados
table(pull(valores_predicts, ATRASADO), pull(valores_predicts, y_hat))
# realizando um previsção se um voo novo vai atrasar ou não pelo modelo
novo_dado_atraso_R <- data.frame(DIA_DA_SEMANA=c("3"),
HORAS=c(2),
MES=c("2"),
DISTANCE=c(2000))
# novo dado transformado para um spark dataframe
novo_dado_atraso_spark <- copy_to(sc, novo_dado_atraso_R, "novo_dado_atraso")
# coletando os fitted values
novo_dado_atraso_predict <- ml_predict(modelo_atrasos_voos, novo_dado_atraso_spark)
# resultado da previsão
print(novo_dado_atraso_predict)
# armazenando o dataset em formato parquet no diretório local
spark_write_parquet(dados_voos_completos,
path = "./datasets/arquivos-comprimidos/voos-avioes.parquet",
mode = "overwrite")
# lendo o arquivo parquet
dados_voos_completo_parquet <- spark_read_parquet(sc,
name = "dados_voos_completo_parquet",
path = "./datasets/arquivos-comprimidos/voos-avioes.parquet")
# visualizando o dataset carregado
glimpse(dados_voos_completo_parquet)
# contando a quantidade de linhas
sdf_nrow(dados_voos_completo_parquet)
# verificar o tempo de processamento para coletar os dados armazenados em parquet
system.time(spark_read_parquet(sc,
name = "dados_voos_completo_parquet",
path = "./datasets/arquivos-comprimidos/voos-avioes.parquet"))
dados_voos_completo_parquet <- dados_voos_completo_parquet %>%
mutate(ANO = as.character(year(FL_DATE)))
# verificando a variável adicionada
glimpse(dados_voos_completo_parquet)
# carregando o dataset em um diretório local e particionando os dados por ano
spark_write_parquet(dados_voos_completo_parquet,
path = "./datasets/arquivos-comprimidos/voos-avioes-anos.parquet",
partition_by = c("ANO"),
mode = "overwrite")
# verificar o tempo de processamento para coletar os dados armazenados em parquet do ano de 2017
system.time(dados_voos_ano_2017_parquet <- spark_read_parquet(sc,
name = "dados_voos_ano_2017_parquet",
path = "./datasets/arquivos-comprimidos/voos-avioes-anos.parquet/ANO=2017"))
# mesmo procedimento para acessar os dados do ano de 2018
system.time(dados_voos_ano_2018_parquet <- spark_read_parquet(sc,
name = "dados_voos_ano_2018_parquet",
path = "./datasets/arquivos-comprimidos/voos-avioes-anos.parquet/ANO=2018"))
dados_voos_completo_parquet <- dados_voos_completo_parquet %>%
mutate(MES = as.character(month(FL_DATE)))
# e possivel adicionar mais particoes quando o arquivo parquet e salvo em algum diretório
spark_write_parquet(dados_voos_completo_parquet,
path = "./datasets/arquivos-comprimidos/voos-avioes-anos-mes.parquet",
partition_by = c("ANO","MES"),
mode = "overwrite")
# verificar o tempo de processamento para coletar os dados de mes 2 de 2017
system.time(dados_voos_ano_2017_2_parquet <- spark_read_parquet(sc,
name = "dados_voos_ano_2017_2_parquet",
path = "./datasets/arquivos-comprimidos/voos-avioes-anos-mes.parquet/ANO=2017/MES=2"))
# verificar o tempo de processamento para coletar os dados de mes 4 de 2018
system.time(dados_voos_ano_2018_4_parquet <- spark_read_parquet(sc,
name = "dados_voos_ano_2018_4_parquet",
path = "./datasets/arquivos-comprimidos/voos-avioes-anos-mes.parquet/ANO=2018/MES=4"))
# quantidade de linhas do dataset completo
sdf_nrow(dados_voos_completo_parquet)
# quantidade de linhas do dataset completo
sdf_nrow(dados_voos_ano_2018_parquet)
# quantidade de linhas do dataset completo
sdf_nrow(dados_voos_ano_2018_4_parquet)
# desconectar alguma conexão ativa com o spark
spark_disconnect_all()
# lista de pacotes necess?rios
pacotes <- c("sparklyr",
"dplyr",
"arrow",
"nlme",
"ggplot2",
"carrier",
"mlflow",
"reticulate",
"stats",
"glue")
# instalar e carregar os pacotes
instalar_carregar_pacotes(pacotes)
# fun??o para instalar e carregar os pacotes necess?rios
instalar_carregar_pacotes <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
# lista de pacotes necess?rios
pacotes <- c("sparklyr",
"dplyr",
"arrow",
"nlme",
"ggplot2",
"carrier",
"mlflow",
"reticulate",
"stats",
"glue")
# instalar e carregar os pacotes
instalar_carregar_pacotes(pacotes)
# desconectar alguma conexão ativa com o spark
spark_disconnect_all()
# conectar ao cluster spark local
sc <- spark_connect(master = "local",
spark_home="./spark/build/spark-3.4.0-bin-hadoop3")
# carregando o dataset em um diretório local e particionando os dados por ano
spark_write_parquet(dados_voos_2017,
path = "./datasets/arquivos-comprimidos/voos-avioes-anos.parquet",
mode = "overwrite")
# carregando o primeiro dataset da base de dados de voos de avioes
dados_voos_2017 <- spark_read_csv(sc,
name="dados_voos_2017",
path="./datasets/voos-avioes/2017.csv",
header=TRUE,
infer_schema=TRUE)
getenv()
Sys.getenv()
# carregando o dataset em um diretório local e particionando os dados por ano
spark_write_parquet(dados_voos_2017,
path = "./datasets/arquivos-comprimidos/voos-avioes-anos.parquet",
partition_by = c("ANO"),
mode = "overwrite")
# carregando o dataset em um diretório local e particionando os dados por ano
spark_write_parquet(dados_voos_2017,
path = "./datasets/arquivos-comprimidos/voos-avioes-anos.parquet",
#partition_by = c("ANO"),
mode = "overwrite")
